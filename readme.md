# Framework for generative models with conditional sampling
 
A library for easy creation, training and conditional resampling using Generative Models in Artificial Intelligence.
  
## How to install and run
1. Clone the repository;
2. Install Python (>= 3.9 recommended) and the dependencies in `requirements.txt` using `python -m pip install -r requirements.txt`

## How to use

Let us instantiate and train a Conditional Variational Autoencoder.
We have already prepared a choice of parameters and stored them in a configuration dictionary. Import this dictionary with

```
from Config_files.Inference.CVAE_for_mixture_gaussians import config
from Base_classes.Models.APIs.CVAE_API import CVAE_API

model = CVAE_API(**config)
```

Each entry of the dataset contains first the data, then the labels. 
We recover the data (with rescaling) and the labels (without rescaling) with
```
from Base_classes.Data_processing.helpers import get_data_from_file

# Imports data which was scaled using the scaler. The labels are not rescaled
train_data_scaled, test_data_scaled, scaler = get_data_from_file(**config)
```

Training is done with the `fit` method of the model, after having set the optimizer (by default, the Adam optimizer is used).

```
model.set_optimizer()
model.fit(train_data_scaled, **config)
```                               

Conditional resampling is done by calling the model.
```
import torch

n_sample = 1000
label = 1
labels = torch.full(size=(n_sample, config["label_dim"]),
                    fill_value=label)

# For each entry of labels, we will generate a corresponding (fake) data. 
# This data needs to be passed through the inverse scaler.
resampled = scaler.inverse_transform(model(labels).detach().numpy())

print(resampled)
```

Finally, we plot the results:

```
from Visualization import plot_data_and_reconstruction

test_data_no_labels = scaler.inverse_transform(test_data_scaled[:, :-config["label_dim"]])

fig = plot_data_and_reconstruction([test_data_no_labels, resampled],
                                   plot_titles=["Test data", "Resampled data"])
fig.show()
```

A more thorough example code is provided in `main.py`.


## Currently implemented models

The available models can be found in `Base_classes/Models/API`. They inherit
from the `Base_classes.Models.Generative_model.GenerativeModel`.

### Conditional Variational Autoencoder

  ![AutoEncoder](https://github.com/maximilian-janisch/Machine-Learning-in-Healthcare-Publication-is-transitory/assets/36985702/5dd89290-34c7-4a9b-840a-8e7e82a8fbd7)

A CVAE first encodes the data and labels (contained in a single vector) into a latent variable. The decoder then takes 
this latent variable and the labels, and outputs reconstructed data.
By feeding the labels to both the encoder and decoder, we are forcing the CVAE to learn relevant features of the data 
given the labels. 

The loss function is composed of two terms: the reconstruction loss (by default the Mean Square Error loss) ensuring 
that the reconstructed data is as similar as possible to the data, 
and the regularization loss ensuring that the latent variable follow a standard normal distribution 
(by default the Kullback—Leibler divergence), ensuring diversity of the generated data. To avoid to get stuck with a
latent variable distributed according to a gaussian, we added an extra term `gamma / KLD` to the loss:
```
loss = mse + beta * kld + gamma / kld
```

Once the CVAE is trained, new data is generated by feeding gaussian noise (together with the labels) to the decoder.


## Credits and license
**Authors:** Maximilian Janisch, Olivia Jullian Parra, Dr. Thomas Lehéricy, Shrija Rajen Sheth, Sara Zoccheddu.

**License:** MIT
